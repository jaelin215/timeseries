{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Time Series Forecasting in Python:","metadata":{}},{"cell_type":"markdown","source":"## Objective:\n\n* Combine, clean, and prepare the energy and temperature datasets for exploration and modeling. We will use the combined and cleaned datasets to make the exploration and modeling an easier task in the upcoming sections.","metadata":{}},{"cell_type":"markdown","source":"## Data Description:\n* hr_temp_20170201-20200131_subset.csv – This is a dataset containing hourly (variable DATE) temperature data (variable HourlyDryBulbTemperature) at a weather station near the area you are forecasting energy for.\n\n* hrl_load_metered - 20170201-20200131.csv – This is a dataset containing hourly (variable datetime_beginning_ept) megawatt usage data (variable mw) for the area in Pennsylvania centered around Duquesne. We are using only three years of data because we want to make sure that we look at recent energy patterns that are still applicable to our current customers.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T00:07:37.779802Z","iopub.execute_input":"2022-03-28T00:07:37.780348Z","iopub.status.idle":"2022-03-28T00:07:37.784349Z","shell.execute_reply.started":"2022-03-28T00:07:37.780312Z","shell.execute_reply":"2022-03-28T00:07:37.783446Z"},"trusted":true},"execution_count":913,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/milestone1'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:38.277100Z","iopub.execute_input":"2022-03-28T00:07:38.277396Z","iopub.status.idle":"2022-03-28T00:07:38.287446Z","shell.execute_reply.started":"2022-03-28T00:07:38.277365Z","shell.execute_reply":"2022-03-28T00:07:38.286786Z"},"trusted":true},"execution_count":914,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 1.0 Load Data:\n* First, let's make sure to import date related values with 'datetime64' data type for easy manipulation.","metadata":{}},{"cell_type":"code","source":"meter = pd.read_csv('/kaggle/input/milestone1/hrl_load_metered - 20170201-20200131.csv')#, parse_dates=['datetime_beginning_utc', 'datetime_beginning_ept'])\nweather = pd.read_csv('/kaggle/input/milestone1/hr_temp_20170201-20200131_subset.csv', parse_dates=['DATE'])\nweather1 = pd.read_csv('/kaggle/input/milestone1/hr_temp_20200201-20200229_subset.csv', parse_dates=['DATE'])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:38.773915Z","iopub.execute_input":"2022-03-28T00:07:38.774371Z","iopub.status.idle":"2022-03-28T00:07:38.887551Z","shell.execute_reply.started":"2022-03-28T00:07:38.774310Z","shell.execute_reply":"2022-03-28T00:07:38.886914Z"},"trusted":true},"execution_count":915,"outputs":[]},{"cell_type":"code","source":"for column in ['datetime_beginning_utc', 'datetime_beginning_ept']:\n    meter[column] = pd.to_datetime(meter[column])\n\nweather = weather.sort_values('DATE')\nmeter = meter.sort_values('datetime_beginning_ept')\nprint(f'weather: {weather.shape}')\nprint(f'meter: {meter.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:39.036816Z","iopub.execute_input":"2022-03-28T00:07:39.037115Z","iopub.status.idle":"2022-03-28T00:07:44.199887Z","shell.execute_reply.started":"2022-03-28T00:07:39.037085Z","shell.execute_reply":"2022-03-28T00:07:44.198912Z"},"trusted":true},"execution_count":916,"outputs":[]},{"cell_type":"code","source":"weather.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.201356Z","iopub.execute_input":"2022-03-28T00:07:44.201601Z","iopub.status.idle":"2022-03-28T00:07:44.210754Z","shell.execute_reply.started":"2022-03-28T00:07:44.201570Z","shell.execute_reply":"2022-03-28T00:07:44.209905Z"},"trusted":true},"execution_count":917,"outputs":[]},{"cell_type":"code","source":"meter.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.212425Z","iopub.execute_input":"2022-03-28T00:07:44.213048Z","iopub.status.idle":"2022-03-28T00:07:44.230612Z","shell.execute_reply.started":"2022-03-28T00:07:44.213002Z","shell.execute_reply":"2022-03-28T00:07:44.229785Z"},"trusted":true},"execution_count":918,"outputs":[]},{"cell_type":"markdown","source":"## 2.0 Preprocessing:","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Drop/Rename columns:\n* Let's drop unnecessary columns and rename some columns for simplicity.","metadata":{}},{"cell_type":"code","source":"#weather\nweather = weather.drop(columns=['STATION','REPORT_TYPE','SOURCE'])","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.233138Z","iopub.execute_input":"2022-03-28T00:07:44.233457Z","iopub.status.idle":"2022-03-28T00:07:44.243221Z","shell.execute_reply.started":"2022-03-28T00:07:44.233416Z","shell.execute_reply":"2022-03-28T00:07:44.242340Z"},"trusted":true},"execution_count":919,"outputs":[]},{"cell_type":"code","source":"#meter\nmeter = meter.rename(columns={'datetime_beginning_ept':'DATE'})\nmeter = meter.drop(columns=['datetime_beginning_utc','nerc_region','mkt_region','zone','load_area','is_verified'])\nmeter.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.244386Z","iopub.execute_input":"2022-03-28T00:07:44.245226Z","iopub.status.idle":"2022-03-28T00:07:44.265054Z","shell.execute_reply.started":"2022-03-28T00:07:44.245190Z","shell.execute_reply":"2022-03-28T00:07:44.263984Z"},"trusted":true},"execution_count":920,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Index DateTime:\n* Let's create extract information (year, month, hour, day, day_of_week) from 'DATE'column.\n* Then, we will create new columns for it.\n* Fianlly, we will make these columns as index for each dataframe.\n* This makes combining our two dataframes (energy & weather) easier using common index.","metadata":{}},{"cell_type":"code","source":"meter['day_of_week'] = meter.DATE.dt.dayofweek\nmeter['hour'] = meter.DATE.dt.hour\nmeter['day'] = meter.DATE.dt.day\nmeter['month'] = meter.DATE.dt.month\nmeter['year'] = meter.DATE.dt.year\nmeter.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.267148Z","iopub.execute_input":"2022-03-28T00:07:44.267501Z","iopub.status.idle":"2022-03-28T00:07:44.301501Z","shell.execute_reply.started":"2022-03-28T00:07:44.267460Z","shell.execute_reply":"2022-03-28T00:07:44.300828Z"},"trusted":true},"execution_count":921,"outputs":[]},{"cell_type":"code","source":"weather['day_of_week'] = weather.DATE.dt.dayofweek\nweather['hour'] = weather.DATE.dt.hour\nweather['day'] = weather.DATE.dt.day\nweather['month'] = weather.DATE.dt.month\nweather['year'] = weather.DATE.dt.year\nweather.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.302792Z","iopub.execute_input":"2022-03-28T00:07:44.303276Z","iopub.status.idle":"2022-03-28T00:07:44.334881Z","shell.execute_reply.started":"2022-03-28T00:07:44.303233Z","shell.execute_reply":"2022-03-28T00:07:44.334278Z"},"trusted":true},"execution_count":922,"outputs":[]},{"cell_type":"code","source":"weather0 = weather.set_index(['year','month','day','hour','day_of_week'])\nmeter0 = meter.set_index(['year','month','day','hour','day_of_week'])\n\nweather0 = weather0.drop(columns=['DATE'])\nmeter0 = meter0.drop(columns=['DATE'])\ndf = weather0.join(meter0, how='outer')\ndf = df.rename(columns={'HourlyDryBulbTemperature':'temp'})\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.336290Z","iopub.execute_input":"2022-03-28T00:07:44.336765Z","iopub.status.idle":"2022-03-28T00:07:44.588540Z","shell.execute_reply.started":"2022-03-28T00:07:44.336723Z","shell.execute_reply":"2022-03-28T00:07:44.587690Z"},"trusted":true},"execution_count":923,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Identify Missing Data:\n* Not all of the temperature data is recorded, as the stations would occasionally not report.\n* So, we will fill in these missing values using linear interpolation.\n* Let's look at what data we are mssing.","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.589624Z","iopub.execute_input":"2022-03-28T00:07:44.589871Z","iopub.status.idle":"2022-03-28T00:07:44.597789Z","shell.execute_reply.started":"2022-03-28T00:07:44.589826Z","shell.execute_reply":"2022-03-28T00:07:44.597177Z"},"trusted":true},"execution_count":924,"outputs":[]},{"cell_type":"code","source":"#missing mw values (Mar 12 2017, Mar 11 2018, Mar 10 2019)\n# Sunday of 3rd week of March may be the yearly maintenance downtime for energy meter\nmissing_mw = df[df['mw'].isna()==True]\nmissing_mw","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.600339Z","iopub.execute_input":"2022-03-28T00:07:44.600798Z","iopub.status.idle":"2022-03-28T00:07:44.614911Z","shell.execute_reply.started":"2022-03-28T00:07:44.600756Z","shell.execute_reply":"2022-03-28T00:07:44.614072Z"},"trusted":true},"execution_count":925,"outputs":[]},{"cell_type":"code","source":"# missing temp values (Oct 25 2018, Dec 25 2018, May 26 2019)\n# weather temp sensor doesn't exibit any particular yearly maintenance downtime.\n# we will fill this temp values with interpolation\nmissing_temp = df[df['temp'].isna()==True]\nmissing_temp","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.616177Z","iopub.execute_input":"2022-03-28T00:07:44.616582Z","iopub.status.idle":"2022-03-28T00:07:44.640132Z","shell.execute_reply.started":"2022-03-28T00:07:44.616553Z","shell.execute_reply":"2022-03-28T00:07:44.639254Z"},"trusted":true},"execution_count":926,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Treat Missing Data:\n* we are missing:\n    * 37 temperature data [temp (unit: degree F)]\n    * 3 energy consumption data [mw (unit: MWh)]\n* Let's use linear interpolation to fill in the gaps with forward direction.","metadata":{}},{"cell_type":"code","source":"df['temp'] = df['temp'].interpolate(method='linear', limit_direction = 'forward')\ndf['mw'] = df['mw'].interpolate(method='linear', limit_direction = 'forward')\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:12:44.071387Z","iopub.execute_input":"2022-03-28T00:12:44.071824Z","iopub.status.idle":"2022-03-28T00:12:44.083639Z","shell.execute_reply.started":"2022-03-28T00:12:44.071792Z","shell.execute_reply":"2022-03-28T00:12:44.083019Z"},"trusted":true},"execution_count":937,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Re-index with Date:\n* we will desolve multi-index (year, month, day, hour)\n* Then, we will create a new index, 'date', which will combine all these values.","metadata":{}},{"cell_type":"code","source":"df = df.reset_index()\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.661273Z","iopub.execute_input":"2022-03-28T00:07:44.661487Z","iopub.status.idle":"2022-03-28T00:07:44.677239Z","shell.execute_reply.started":"2022-03-28T00:07:44.661460Z","shell.execute_reply":"2022-03-28T00:07:44.676334Z"},"trusted":true},"execution_count":928,"outputs":[]},{"cell_type":"code","source":"df['date'] = pd.to_datetime(dict(year=df.year, month=df.month, day=df.day, hour=df.hour))\nprint(df.dtypes)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.678136Z","iopub.execute_input":"2022-03-28T00:07:44.678687Z","iopub.status.idle":"2022-03-28T00:07:44.705809Z","shell.execute_reply.started":"2022-03-28T00:07:44.678653Z","shell.execute_reply":"2022-03-28T00:07:44.704937Z"},"trusted":true},"execution_count":929,"outputs":[]},{"cell_type":"code","source":"df = df.set_index('date')\ndf = df.drop(columns='day')\ndf = df.rename(columns={'day_of_week':'weekday'})\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.707769Z","iopub.execute_input":"2022-03-28T00:07:44.708122Z","iopub.status.idle":"2022-03-28T00:07:44.725546Z","shell.execute_reply.started":"2022-03-28T00:07:44.708077Z","shell.execute_reply":"2022-03-28T00:07:44.724924Z"},"trusted":true},"execution_count":930,"outputs":[]},{"cell_type":"markdown","source":"### 2.6 Train/Test Split:\n* train = from 2017-02-01 to 2019-12-31\n* test = from 2020-01-01 to 2020-01-31","metadata":{}},{"cell_type":"code","source":"train = df[df.index.to_series().between('2017-01-01 00:00:00','2019-12-31 23:59:59') == True]\ntest = df[df.index.to_series().between('2020-01-01 00:00:00','2020-12-31 23:39:59') == True]","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.726550Z","iopub.execute_input":"2022-03-28T00:07:44.726963Z","iopub.status.idle":"2022-03-28T00:07:44.736607Z","shell.execute_reply.started":"2022-03-28T00:07:44.726929Z","shell.execute_reply":"2022-03-28T00:07:44.735950Z"},"trusted":true},"execution_count":931,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.738323Z","iopub.execute_input":"2022-03-28T00:07:44.738965Z","iopub.status.idle":"2022-03-28T00:07:44.762340Z","shell.execute_reply.started":"2022-03-28T00:07:44.738914Z","shell.execute_reply":"2022-03-28T00:07:44.761381Z"},"trusted":true},"execution_count":932,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.764012Z","iopub.execute_input":"2022-03-28T00:07:44.764338Z","iopub.status.idle":"2022-03-28T00:07:44.780536Z","shell.execute_reply.started":"2022-03-28T00:07:44.764295Z","shell.execute_reply":"2022-03-28T00:07:44.779920Z"},"trusted":true},"execution_count":933,"outputs":[]},{"cell_type":"code","source":"# let's confirm that we didn't miss any rows when splitting dataset to train/test\nprint(f'df: {df.shape[0]}')\nprint(f'train: {train.shape[0]}')\nprint(f'test: {test.shape[0]}')\nprint(f'train+test = df: {train.shape[0]+test.shape[0]} = {df.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T00:07:44.781890Z","iopub.execute_input":"2022-03-28T00:07:44.782635Z","iopub.status.idle":"2022-03-28T00:07:44.795686Z","shell.execute_reply.started":"2022-03-28T00:07:44.782590Z","shell.execute_reply":"2022-03-28T00:07:44.794736Z"},"trusted":true},"execution_count":934,"outputs":[]},{"cell_type":"markdown","source":"## Remark:\n* Train (25539)/Test(744) split is successfully completed with no data loss.\n* Now, we are ready to submit this assignment.","metadata":{}},{"cell_type":"markdown","source":"### ","metadata":{}}]}
